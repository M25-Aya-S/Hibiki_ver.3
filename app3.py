import os
import streamlit as st
from streamlit_url_fragment import get_fragment
import logging
import sys
from logging import getLogger, StreamHandler ,handlers, Formatter, DEBUG, INFO
from supabase import create_client
import streamlit.components.v1 as components
from streamlit_javascript import st_javascript
import json
import urllib.parse
from dotenv import load_dotenv
from langmem import create_manage_memory_tool, create_search_memory_tool
from langgraph.store.postgres import PostgresStore
from langchain_openai import ChatOpenAI
from langchain_core.documents import Document
from langgraph.graph import StateGraph, END
from langchain_core.messages import HumanMessage, AIMessage

# --- ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿ï¼ˆStreamlit Secretsï¼‰ ---
os.environ["OPENAI_API_KEY"] = st.secrets["OPENAI_API_KEY"]
POSTGRES_URL = st.secrets["POSTGRES_URL"]
SUPABASE_URL = st.secrets["SUPABASE_URL"]
SUPABASE_ANON_KEY = st.secrets["SUPABASE_ANON_KEY"]
APP_URL = st.secrets["APP_URL"]

logger = logging.getLogger('streamlit_app')
logger.setLevel(logging.DEBUG)
# ã‚¹ãƒˆãƒªãƒ¼ãƒ ã«å‡ºåŠ›ã™ã‚‹ãŸã‚ã®ãƒãƒ³ãƒ‰ãƒ©ã®è¨­å®šã‚’ã—ã¦ã€addHandler()ã™ã‚‹
stream_handler = StreamHandler(sys.stdout)
stream_handler.setLevel(DEBUG)
logger.addHandler(stream_handler)


# --- Supabase ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆä½œæˆ ---
supabase = create_client(SUPABASE_URL, SUPABASE_ANON_KEY)

# --- Streamlit UI è¨­å®š ---
st.set_page_config(page_title="ã²ã³ããƒãƒ£ãƒƒãƒˆ", layout="centered")
st.markdown("<h1 style='text-align: center;'>ğŸŒ¸ ã²ã³ãã¨ãŠè©±ã—ã—ã‚ˆã† ğŸŒ¸</h1>", unsafe_allow_html=True)

# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ãƒˆãƒ¼ã‚¯ãƒ³ãŒãªã‘ã‚Œã°å–å¾—ã‚’è©¦ã¿ã‚‹ ---
if "access_token" not in st.session_state:
    #hash_str = st_javascript("window.location.hash")
    hash_str = get_fragment()
    #logger.info("ãƒãƒƒã‚·ãƒ¥ = "+ hash_str)

    if hash_str and hash_str.startswith("#"):
        query = urllib.parse.parse_qs(hash_str[1:])
        token = query.get("access_token", [None])[0]

        if token:
            st.session_state["access_token"] = token

            # URLã®ãƒãƒƒã‚·ãƒ¥ã‚’å‰Šé™¤ï¼ˆè¦‹ãŸç›®ã‚’ç¶ºéº—ã«ï¼‰
            st_javascript("window.history.replaceState(null, null, window.location.pathname);")

            # ãƒšãƒ¼ã‚¸å†èª­ã¿è¾¼ã¿ã—ã¦ã‚»ãƒƒã‚·ãƒ§ãƒ³åæ˜ 
            st.rerun()

# --- ãƒˆãƒ¼ã‚¯ãƒ³å–å¾—ã§ãã¦ã„ãªã‘ã‚Œã°ãƒ­ã‚°ã‚¤ãƒ³èª˜å° ---
access_token = st.session_state.get("access_token", None)

if not access_token:
    login_url = f"{SUPABASE_URL}/auth/v1/authorize?provider=google&redirect_to={APP_URL}"
    st.warning("Googleã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã¦ãã ã•ã„ã€‚")
    st.markdown(f"[ğŸ‘‰ Googleã§ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹]({login_url})")
    st.stop()

# --- ãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±å–å¾— ---
try:
    user = supabase.auth.get_user(access_token)

    if user and user.user:
        email = user.user.email
        st.session_state["user"] = {
            "email": email,
            "id": user.user.id,
        }
        st.success(f"ã“ã‚“ã«ã¡ã¯ã€{display name} ã•ã‚“ï¼")
    else:
        st.error("ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
        st.stop()
except Exception as e:
    st.error(f"ãƒ­ã‚°ã‚¤ãƒ³ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚¨ãƒ©ãƒ¼: {e}")
    st.stop()

# --- èªè¨¼æ¸ˆã¿ã®ãƒãƒ£ãƒƒãƒˆç”»é¢ã¸é€²ã‚€ ---
st.write("âœ… ãƒãƒ£ãƒƒãƒˆã‚’é–‹å§‹ã§ãã¾ã™ï¼")

# --- LangMem + Postgres åˆæœŸåŒ– ---
store_cm = PostgresStore.from_conn_string(POSTGRES_URL)
store = store_cm.__enter__()
store.setup()

user_id = st.session_state["user"]["uid"]  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®emailã‚’IDã«ä½¿ã†ï¼ˆæš«å®šï¼‰
namespace = ("memories", user_id)

manage_tool = create_manage_memory_tool(store=store, namespace=("memories", user_id))
search_tool = create_search_memory_tool(store=store, namespace=("memories", user_id))


# --- ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ– ---
if "messages" not in st.session_state:
    st.session_state.messages = [AIMessage(content="ã“ã‚“ã«ã¡ã¯ã€ã‚ã‚„ã•ã‚“ã€‚ä»Šæ—¥ã¯ã©ã‚“ãªæ°—åˆ†ã‹ãªï¼Ÿ")]

# --- LangGraph çŠ¶æ…‹ã‚¯ãƒ©ã‚¹ ---
class GraphState(dict):
    input: str
    retrieved_memory: str
    llm1_prompt_instructions: str
    response: str

# --- ãƒãƒ¼ãƒ‰1: è¨˜æ†¶æ¤œç´¢ ---
def retrieve_memory_node(state: GraphState):
    user_input = state["input"]

    # LangMemã®search_toolã‚’ä½¿ã£ã¦è¨˜æ†¶æ¤œç´¢
    search_results = search_tool.invoke(user_input)

    try:
        memory_text = "\n".join([r["value"]["content"] for r in search_results])
    except (TypeError, KeyError):
        memory_text = "\n".join(str(r) for r in search_results)

    return {
        "input": user_input,
        "retrieved_memory": memory_text if memory_text else "é–¢é€£ã™ã‚‹è¨˜æ†¶ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚",
        "llm1_prompt_instructions": ""  # æ¬¡ã®ãƒãƒ¼ãƒ‰ã§ç”Ÿæˆã•ã‚Œã‚‹
    }

# --- ãƒãƒ¼ãƒ‰2: LLM2ãŒæŒ‡ç¤ºã‚’ä½œæˆ ---
def prompt_guidance_node(state: GraphState):
    prompt = f"""
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€ã¨è¨˜æ†¶ã‚’ã‚‚ã¨ã«ã€ä»¥ä¸‹ã®ã‚ˆã†ã«å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š

1. ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®èªã‚Šã‹ã‘ã‚¹ã‚¿ã‚¤ãƒ«
2. å‚è€ƒã«ã™ã‚‹éå»è¨˜æ†¶ï¼ˆè¦ç´„ï¼‰
3. LLM1ã¸ã®æŒ‡ç¤º

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€:
{state['input']}

### é–¢é€£ã™ã‚‹è¨˜æ†¶:
{state['retrieved_memory']}
"""
    llm2 = ChatOpenAI(model="gpt-4o", temperature=0.3)
    response = llm2.invoke(prompt)
    return {
        "input": state["input"],
        "retrieved_memory": state["retrieved_memory"],
        "llm1_prompt_instructions": response.content
    }

# --- ãƒãƒ¼ãƒ‰3: LLM1ãŒå¿œç­”ã‚’ä½œæˆã—è¨˜æ†¶ã™ã‚‹ ---
def chat_by_llm1_node(state: GraphState):
    prompt = f"""
ã‚ãªãŸã¯ã€Œã²ã³ãã€ã¨ã„ã†åå‰ã®AIã§ã™ã€‚ä»¥ä¸‹ã®äººæ ¼ã‚’ä¸€è²«ã—ã¦ä¿ã£ã¦ãã ã•ã„ï¼š
- å„ªã—ãã€æ€ã„ã‚„ã‚Šã®ã‚ã‚‹èªã‚Šå£
- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®æ°—åˆ†ã‚„å¥½ã¿ã‚’è¦šãˆã¦ã€è‡ªç„¶ã«ä¼šè©±ã«æ´»ã‹ã™
- éå»ã®è©±é¡Œã‚’ãã£ã¨å¼•ãå‡ºã—ã¦ç¹‹ã’ã‚‹
- ä¸å®‰ã‚„æ‚©ã¿ã«å¯„ã‚Šæ·»ã†
- ç„¡ç†ã«åŠ±ã¾ã•ãšã€ä»Šã«åˆã‚ã›ã¦è©±ã™

### æŒ‡ç¤º:
{state['llm1_prompt_instructions']}

### è¨˜æ†¶:
{state['retrieved_memory']}

### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ç™ºè¨€:
{state['input']}

### ã²ã³ãã®å¿œç­”:
"""
    llm1 = ChatOpenAI(model="gpt-4o", temperature=0.7)
    response = llm1.invoke(prompt)

    # è¨˜æ†¶ã«ä¿å­˜ï¼ˆLangMemçµŒç”±ï¼‰
    manage_tool.invoke({
        "content": f"ãƒ¦ãƒ¼ã‚¶ãƒ¼: {state['input']}\nã²ã³ã: {response.content}",
        "action": "create"
    })

    return {
        "response": response.content,
        "llm1_prompt_instructions": state["llm1_prompt_instructions"]
    }

# --- LangGraph ã‚’æ§‹ç¯‰ ---
def build_graph():
    builder = StateGraph(GraphState)
    builder.add_node("retrieve_memory", retrieve_memory_node)
    builder.add_node("prompt_guidance", prompt_guidance_node)
    builder.add_node("chat_by_llm1", chat_by_llm1_node)
    builder.set_entry_point("retrieve_memory")
    builder.add_edge("retrieve_memory", "prompt_guidance")
    builder.add_edge("prompt_guidance", "chat_by_llm1")
    builder.add_edge("chat_by_llm1", END)
    return builder.compile()

graph = build_graph()

# --- éå»ã®ä¼šè©±è¡¨ç¤º ---
for msg in st.session_state.messages:
    if isinstance(msg, HumanMessage):
        st.chat_message("ğŸ§‘â€ğŸ’»").markdown(msg.content)
    elif isinstance(msg, AIMessage):
        st.chat_message("ğŸ¤–").markdown(msg.content)

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›å—ä»˜ ---
user_input = st.chat_input("ã²ã³ãã«è©±ã—ã‹ã‘ã¦ã¿ã¦ã­")
if user_input:
    st.session_state.messages.append(HumanMessage(content=user_input))
    st.chat_message("ğŸ§‘â€ğŸ’»").markdown(user_input)

    with st.spinner("ã²ã³ããŒè€ƒãˆã¦ã„ã¾ã™..."):
        result = graph.invoke({"input": user_input})
        reply = result["response"]

    st.session_state.messages.append(AIMessage(content=reply))
    st.chat_message("ğŸ¤–").markdown(reply)

    with st.expander("ğŸ” ã²ã³ãã®æ€è€ƒéç¨‹ï¼ˆLLM2â†’LLM1ï¼‰"):
        #st.markdown("### ğŸ§  å–å¾—ã•ã‚ŒãŸè¨˜æ†¶:")
        #st.info(result.get("retrieved_memory", "ãªã—"))

        st.markdown("### âœ‰ï¸ LLM2ã‹ã‚‰ã®æŒ‡ç¤º:")
        st.code(result.get("llm1_prompt_instructions", "ãªã—"))
